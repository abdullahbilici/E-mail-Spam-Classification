{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-Mail Spam Classification\n",
    "## YZV 311E Term Project\n",
    "\n",
    "Abdullah Bilici, 150200330\n",
    "\n",
    "Bora Boyacıoğlu, 150200310"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Load the CSV data to a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mails = pd.read_csv(\"../Data/emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sice the data is too large, using a ~1/20 batch would be easier for test purposes.\n",
    "mails_full = mails.copy()\n",
    "mails = mails.iloc[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Using an English NLP model, tokenise the sentences for each mail. Then, apply some rules to make the data workable. These include:\n",
    "\n",
    "* Tokenisation\n",
    "1. Lowercasing\n",
    "2. Stop word removal\n",
    "3. Special character removal\n",
    "4. Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Get the stop words and punctuations\n",
    "stop_words = set(nlp.Defaults.stop_words)\n",
    "punctuations = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for tokenising and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_sentence(sentence):\n",
    "    # Error handling for non-string inputs\n",
    "    if not isinstance(sentence, str):\n",
    "        return []\n",
    "    \n",
    "    # Tokenise the sentence\n",
    "    tokenised = nlp(sentence)\n",
    "    \n",
    "    # Lowercase and lemmatise the words\n",
    "    tokens = [token.lemma_.lower().strip() if token.pos_ != \"PRON\" else token.lower_ for token in tokenised]\n",
    "    \n",
    "    # Remove stop words and special characters\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in punctuations]\n",
    "    \n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if token != '']\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply tokenisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mails['tokenised_text'] = mails['text'].apply(tokenise_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [subject, naturally, irresistible, corporate, ...\n",
       "1    [subject, stock, trading, gunslinger, fanny, m...\n",
       "2    [subject, unbelievable, new, home, easy, m, wa...\n",
       "3    [subject, 4, color, printing, special, request...\n",
       "4    [subject, money, software, cd, software, compa...\n",
       "Name: tokenised_text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mails['tokenised_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subject',\n",
       " 'naturally',\n",
       " 'irresistible',\n",
       " 'corporate',\n",
       " 'identity',\n",
       " 'lt',\n",
       " 'hard',\n",
       " 'recollect',\n",
       " 'company',\n",
       " 'market',\n",
       " 'suqgestion',\n",
       " 'information',\n",
       " 'isoverwhelminq',\n",
       " 'good',\n",
       " 'catchy',\n",
       " 'logo',\n",
       " 'stylish',\n",
       " 'statlonery',\n",
       " 'outstanding',\n",
       " 'website',\n",
       " 'task',\n",
       " 'easy',\n",
       " 'promise',\n",
       " 'havinq',\n",
       " 'order',\n",
       " 'iogo',\n",
       " 'company',\n",
       " 'automaticaily',\n",
       " 'world',\n",
       " 'ieader',\n",
       " 'isguite',\n",
       " 'ciear',\n",
       " 'good',\n",
       " 'product',\n",
       " 'effective',\n",
       " 'business',\n",
       " 'organization',\n",
       " 'practicable',\n",
       " 'aim',\n",
       " 'hotat',\n",
       " 'nowadays',\n",
       " 'market',\n",
       " 'promise',\n",
       " 'marketing',\n",
       " 'effort',\n",
       " 'effective',\n",
       " 'list',\n",
       " 'clear',\n",
       " 'benefit',\n",
       " 'creativeness',\n",
       " 'hand',\n",
       " 'original',\n",
       " 'logo',\n",
       " 'specially',\n",
       " 'reflect',\n",
       " 'distinctive',\n",
       " 'company',\n",
       " 'image',\n",
       " 'convenience',\n",
       " 'logo',\n",
       " 'stationery',\n",
       " 'provide',\n",
       " 'format',\n",
       " 'easy',\n",
       " 'use',\n",
       " 'content',\n",
       " 'management',\n",
       " 'system',\n",
       " 'letsyou',\n",
       " 'change',\n",
       " 'website',\n",
       " 'content',\n",
       " 'structure',\n",
       " 'promptness',\n",
       " 'logo',\n",
       " 'draft',\n",
       " 'business',\n",
       " 'day',\n",
       " 'affordability',\n",
       " 'marketing',\n",
       " 'break',\n",
       " 'shouldn',\n",
       " 't',\n",
       " 'gap',\n",
       " 'budget',\n",
       " '100',\n",
       " 'satisfaction',\n",
       " 'guarantee',\n",
       " 'provide',\n",
       " 'unlimited',\n",
       " 'change',\n",
       " 'extra',\n",
       " 'fee',\n",
       " 'surethat',\n",
       " 'love',\n",
       " 'result',\n",
       " 'collaboration',\n",
       " 'look',\n",
       " 'portfolio',\n",
       " 'interested']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mails.iloc[0]['tokenised_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Modelling\n",
    "\n",
    "Transforming the text into numerical format and vectorising them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words (BoW)\n",
    "The Bag-of-Words model represents text as an unordered collection of words frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Join the tokens into corpus for vectorisation\n",
    "corpus = [' '.join(tokens) for tokens in mails['tokenised_text']]\n",
    "\n",
    "# Vectorise the corpus\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "TF-IDF considers the frequency of the word in the sentence in relation to the frequency in the corpus, helping to diminish the importance of frequently occurring words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorisation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sparse matrix to a dataframe\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>0004</th>\n",
       "      <th>00076</th>\n",
       "      <th>0052</th>\n",
       "      <th>00971</th>\n",
       "      <th>01</th>\n",
       "      <th>0100</th>\n",
       "      <th>01019</th>\n",
       "      <th>...</th>\n",
       "      <th>zmsx</th>\n",
       "      <th>znalazlam</th>\n",
       "      <th>zndnioay</th>\n",
       "      <th>zoloftpain</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoolant</th>\n",
       "      <th>zuid</th>\n",
       "      <th>zxghlajf</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.003750</td>\n",
       "      <td>0.009948</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.017907</td>\n",
       "      <td>0.033738</td>\n",
       "      <td>0.031841</td>\n",
       "      <td>0.003161</td>\n",
       "      <td>0.003161</td>\n",
       "      <td>0.006716</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.013906</td>\n",
       "      <td>0.017668</td>\n",
       "      <td>0.006322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004976</td>\n",
       "      <td>0.011110</td>\n",
       "      <td>0.011899</td>\n",
       "      <td>0.008295</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>0.018928</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>0.003835</td>\n",
       "      <td>0.002841</td>\n",
       "      <td>0.009208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.119739</td>\n",
       "      <td>0.190038</td>\n",
       "      <td>0.292068</td>\n",
       "      <td>0.049977</td>\n",
       "      <td>0.049977</td>\n",
       "      <td>0.106184</td>\n",
       "      <td>0.094587</td>\n",
       "      <td>0.141094</td>\n",
       "      <td>0.279362</td>\n",
       "      <td>0.099954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078676</td>\n",
       "      <td>0.175664</td>\n",
       "      <td>0.188132</td>\n",
       "      <td>0.131153</td>\n",
       "      <td>0.111565</td>\n",
       "      <td>0.299283</td>\n",
       "      <td>0.104870</td>\n",
       "      <td>0.060639</td>\n",
       "      <td>0.044923</td>\n",
       "      <td>0.145589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 5903 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               00         000        0000        0004       00076        0052  \\\n",
       "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
       "mean     0.003750    0.009948    0.003794    0.000200    0.000200    0.000425   \n",
       "std      0.017907    0.033738    0.031841    0.003161    0.003161    0.006716   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "max      0.119739    0.190038    0.292068    0.049977    0.049977    0.106184   \n",
       "\n",
       "            00971          01        0100       01019  ...        zmsx  \\\n",
       "count  250.000000  250.000000  250.000000  250.000000  ...  250.000000   \n",
       "mean     0.000378    0.001895    0.001117    0.000400  ...    0.000315   \n",
       "std      0.005982    0.013906    0.017668    0.006322  ...    0.004976   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "max      0.094587    0.141094    0.279362    0.099954  ...    0.078676   \n",
       "\n",
       "        znalazlam    zndnioay  zoloftpain        zone         zoo     zoolant  \\\n",
       "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
       "mean     0.000703    0.000753    0.000525    0.000582    0.001197    0.000419   \n",
       "std      0.011110    0.011899    0.008295    0.007368    0.018928    0.006633   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "max      0.175664    0.188132    0.131153    0.111565    0.299283    0.104870   \n",
       "\n",
       "             zuid    zxghlajf        zzzz  \n",
       "count  250.000000  250.000000  250.000000  \n",
       "mean     0.000243    0.000180    0.000582  \n",
       "std      0.003835    0.002841    0.009208  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000  \n",
       "50%      0.000000    0.000000    0.000000  \n",
       "75%      0.000000    0.000000    0.000000  \n",
       "max      0.060639    0.044923    0.145589  \n",
       "\n",
       "[8 rows x 5903 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
